# Twitter

* Whimsical Board [link](https://whimsical.com/twitter-63T4oateDAL6ZdsaJ5LdHT)
* Video [link](https://www.youtube.com/watch?v=wYk0xPP\_P\_8\&ab\_channel=TechDummiesNarendraL)

#### #1. Requirement Gathering =============================================

* **1.1 Functional Requirements**
  * tweet
    * text
    * photo
    * video
  * **trends**
    * see all the tending topics/hastags
  * follow others
  * timeline
    * home timeline(whom user follows)
    * user timeline(user history)
    * search timeline(all tweets as per searched keyword)
* **1.2 NonFunctional Requirements** - Usage Patterns(read heavy/CAP tradeoffs)
  * Highly available
  * Acceptable latency for timeline generation
  * Consistency can take a hit:
    * if a user doesnâ€™t see a tweet for a while, it should be fine.
    * \*\*Eventual Consistency \*\*
  * User traffic will be distributed unevenly throughout the day
* **1.3 Out of Scope**
  * replying on tweets
  * tweet notification
  * suggestion - whom to follow
  * user tagging

#### #2. Back-of-the-envelope estimation =========================================

* **2.1 Scale of System**
  * Daily Active Users(DAU) : 200M
  * write: 2 tweets per user per day
    * \=> daily writes = 2\*200M = 400M/day
  * read:write = 1000:1
    * \=> daily reads = 400B/day
    * system is read-heavy
* **2.2 Storage size estimation**
  * **Text**: 140 chars a tweet
    * 2 bytes to store 1 char (w/o any compression)
    * \+30 bytes for tweet metadata(timestamp, location, userID)
    * \=> 400M\*(280b+30b) = 120GB/day
  * **photo**: 1 in 5 tweets has a photo
    * size of 1 photo = 200KB
    * \=> 400M\*200KB/5 = 400M\*40KB = 16MB\*M = 16TB
  * **video**: 1 in 10 tweets has a video
    * size of 1 video = 2MB
    * \=> 400M\*2MB/10 = 80TB
  * **=> Total daily storage reqd** = 120GB + 16TB + 80TB
    * \= 96.12 TB/day
    * \=> 1.11 GB/sec
* **2.3 Bandwidth estimation**
  * view tweets:
    * **text**:
      * \=> text: 28B\*280b = 7840GB/day = **90MB/sec**
    * user see all the **photos** of every tweet
      * \=> 28B/5\*200KB = 13GB/s
    * user plays 1 in every 3 **videos** of tweet
      * \=> (28B/10/3)\*2MB = 22GB/s
    * **=> total** = 35Gb/s

#### #3. APIs =======================================================

* **tweet**(api\_dev\_key, tweet\_data, tweet\_location, user\_location, media\_ids)
  * \=> returns tweet url to view
* **search**(api\_dev\_key, search\_terms, maximum\_results\_to\_return, sort, page\_token)
  * \=> JSON containing information about a list of tweets

#### #4. Table Schemas & DB Choices=====================

* **Tables:**
  * **User**
    * userID : PK, int
    * name
    * email
    * pwd(hashed)
    * 3rd party login OAuth
    * last login
  * **Tweet**
    * tweetID: PK,int
    * userID
    * text
    * createdAt(ts)
    * locationCordinates
    * mediaAddress
  * **Follower**
    * followerID : FK
    * followeeID : FK
    * timestamp
* **Relations:**
  * 1-Many in User ---- Tweet
  * 1-Many in User ---- Follower
* \[?] Retweets:
* are to be considered a unique tweet & store in 'Tweet' table itself
* **DB choice:**
  * photos+vidoes => DFS
    * HDFS/S3
    * can be queued
  * User, Tweet, Follower tables => NoSQL
    * Distributed NoSQL
    * \=> RDBMS, though good for relations here wont be scalable enough
  * Relationship b/w tables: use\*\* key-value DB \*\*i.e. **Redisâœ…**
    * `<userID> -> [tweetIDs]`
    * `<userID> -> [followerIDs]`

#### #5. Draw 'Basic HLD'=============================================

![](../../.gitbook/assets/screenshot-2021-08-28-at-1.25.03-pm.png)

#### #6. EVOLVE & 'Draw Detailed HLD' with the discussion on each comp =====

1. \*\* How User Timeline is generated?\*\*
   * Go to Redis table#1 : \<userID> -> \[tweetIDs]
   * SCALING UP: **cache**
2. **How to generate Home Timeline**
   * **Steps using Join**
     * get all followers of the user
     * get their latest tweets
     * merge, sort & display
   * **SCALING UP:**
   * âŒShard/NoSQL/multiple nodes => wont be as fast as twitter
   * âœ… use "**Fanout**"
   * **Fanout**
     * Fanout => when a new tweet is generated; process it & DISTRIBUTE it to user timelines
     * **WORKING**:
       1. user makes a new tweet
       2. Store it in Tweets table
       3. Push this tweet into his 'User Timeline'
       4. Fan-out this tweet into 'Home Timeline' of ALL his followers
     * Hence; no DB query is required!!!
     * **NOTE**: Do not do this precomputation for **'Passive users'**

![](../../.gitbook/assets/screenshot-2021-08-28-at-12.42.25-pm.png)

```
3\. **How to handle celebrity tweets?**
```

* celebrity has MILLIONS of followers
*   **WORKING:**

    * **Celebrity side:**
      1. celebrity makes a tweet
      2. Store it in his Tweets table
      3. Push it to his 'User Timeline'
      4. **DO NOT FAN OUT**
    * **Fan Side:**
    * Every fan has a list of celebrities he follows
    * When the fan opens his 'Home Timeline':
      * Go to all the celebrities he follows & check their latet tweets
      * \=> All these operations are Redis operations => wont take much time
    * Store these latest celebrity tweets in a separate sorted list
    * Show these tweets along with users timeline tweets in his 'Home Timeline'
    * **NOTE**: Do not do this precomputation for **'Passive users'**

    ***

![](../../.gitbook/assets/screenshot-2021-08-28-at-12.44.37-pm.png)

#### 4.\*\* Generate #Trends\*\*

* How to see if a topic is trending?
  * It has more #tweets in short span
  * i.e. topic with 1000 tweets in 5 min is a #TredningTopic
  * But a topic with 10,000 tweets in 1 month is #NotATrendingTopic
  * Eg.: election result, movie, cricket store, oplymics/game
* **HOW TO GENERATE:**
  * Collect & Realtime process all tweets
    * Filter out redundant Hashtags (#Enjoy, #Life, #YOLO, #FOOD etc)
    * Policy violation
    * Copy-write violation
  * using \*\*Kafka+Spark \*\*
  * Store it in Trending Db(Redis)

#### 5. **Twitter Search Timeline | IndexingðŸš€ðŸš€**

* Twitter uses **EarlyBird**
  * \=> inverted full text indexing
* **HOW:**
  1. Breakdown/stemming every tweet into **keywords**
  2. Store all these keywords in a DISTRIBUTED BIG TABLE (mapped with their resp. tweetIDs)
  3. **Map/Reduce** works here
* **Scaling:**
  * \*\*Sharding \*\*based on keyword / Location
  * **Caching**
  * \*\*Ranking \*\*by popularity